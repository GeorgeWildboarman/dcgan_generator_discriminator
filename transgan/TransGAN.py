import os
import time
import numpy as np

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.python.ops.array_ops import Identity
from tensorflow.keras import mixed_precision

from IPython import display

dtype_global = tf.float32
# dtype_global = tf.float16

physical_devices = tf.config.list_physical_devices('GPU')
if physical_devices:
    for device in physical_devices:
      print(device)
      tf.config.experimental.set_memory_growth(device, True)
      print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))
      # Mixed precision Setting the dtype policy
      print('Apply Mixed Precision: mixed_float16')
      policy = mixed_precision.Policy('mixed_float16')
      mixed_precision.set_global_policy(policy)
else:
  print("Not enough GPU hardware devices available")

def printGUPMemoryUsage():
  physical_devices = tf.config.list_physical_devices('GPU')
  if physical_devices:
    for device in physical_devices:
      print('****** GPU Memory Usage: {:>16,d} bytes'.format(tf.config.experimental.get_memory_info('GPU:0')['current']))
  # else:
      # print("Not enough GPU hardware devices available")

def load_image(image_path, channels=3):
  """Loads and preprocesses images.
  arguments:
    image_path: String. File path or url to read from. If set url, the url must start with "http" or "https.
    channels: Int. Number of channels of images.
  """
  if 'http' in image_path:
    # Get image file from url and cache locally.
    image_path = tf.keras.utils.get_file(os.path.basename(image_path)[-128:], image_path)

  # Load and convert to float32 numpy array, and normalize to range [0, 1].
  image = tf.io.decode_image(tf.io.read_file(image_path), channels=channels, dtype=dtype_global)
  # Normalize to range [-1, 1]
  image = image*2.0-1.0
  return image

def augmentation_image(image, image_size=(64, 64), expand=1.1):
  expand_size = [int(x*expand) for x in image_size]
  image = tf.image.resize(image, expand_size)
  image_shape = image_size + (image.shape[-1],)
  image = tf.image.random_crop(value=image, size=image_shape)
  image = tf.image.random_flip_left_right(image)
  return image

def load_and_preprocessing_data(path_list, image_size=(64, 64), batch_size=64, channels=3, aug_num=5, expand=1.1):
  prep_images=[]
  print('Number of files to load:', len(path_list))
  for path in path_list:
    image = load_image(path, channels)
    for _ in range(aug_num):
      prep_image =augmentation_image(image, image_size, expand)
      prep_images.append(prep_image)

  print('Number of augmented images:',len(prep_images))
  buffer_size = len(prep_images)

  dataset = tf.data.Dataset.from_tensor_slices(prep_images)
  dataset = dataset.shuffle(buffer_size).batch(batch_size)
  dataset = dataset.map(lambda x: tf.cast(x, dtype_global))
  print('Batch size:',batch_size)
  print('Num batchs', len(dataset))

  return dataset

class ganTraining():
  '''A Generative Adversarial Network (GAN) is a type of machine learning model used for generative tasks,
  such as generating new data samples that resemble a given training dataset. GANs were first introduced by
  Ian Goodfellow and his colleagues in 2014.
   A GAN consists of two neural networks: the generator and the discriminator. These two networks are trained
  together in a competitive process, where the generator tries to create realistic data, and the discriminator
  tries to distinguish between real data and fake data generated by the generator.


  '''

  def __init__(self,
               generator, # Tf.Model, generator model.
               discriminator, #Tf.Model, discriminator model.
               latent_dim = 100, # Dimension of random noise (latent space vectors)
              #  learning_rate = 0.001, # Learning rate for the discriminator and the generator optimizers
              #  beta_1 = 0.9,
               learning_rate = 0.0002, # Learning rate for the discriminator and the generator optimizers
               beta_1 = 0.5,
               beta_2 = 0.999,
               checkpoint_prefix = None,
               ):
    # learning_rate=0.001,
    # beta_1=0.9,
    # beta_2=0.999,
    # epsilon=1e-07,

    self.latent_dim = latent_dim
    self.checkpoint_prefix = checkpoint_prefix

    self.generator = generator
    self.discriminator = discriminator

    self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

    # Define the optimizers
    # The discriminator and the generator optimizers are different since you will train two networks separately.
    self.discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)
    self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)

    self.checkpoint = tf.train.Checkpoint(
        generator_optimizer=self.generator_optimizer,
        discriminator_optimizer=self.discriminator_optimizer,
        generator=self.generator,
        discriminator=self.discriminator)

  # Define the discriminator loss function
  def discriminator_loss(self, real_output, fake_output):
      real_loss = self.cross_entropy(tf.ones_like(real_output, dtype=dtype_global), real_output)
      fake_loss = self.cross_entropy(tf.zeros_like(fake_output, dtype=dtype_global), fake_output)
      total_loss = real_loss + fake_loss
      return total_loss

  # Define the generator loss function
  def generator_loss(self, fake_output):
      return self.cross_entropy(tf.ones_like(fake_output, dtype=dtype_global), fake_output)

  def save(self, checkpoint_prefix=None):
    checkpoint_prefix = checkpoint_prefix or self.checkpoint_prefix
    if checkpoint_prefix:
      return self.checkpoint.save(file_prefix=checkpoint_prefix)
    else:
      return None

  def restore(self, save_path):
    return self.checkpoint.restore(save_path)

  # Define the training step
  @tf.function
  def train_step(self, images):
      # Generate random noise
      batch_size = images.shape[0]
      noise = tf.random.normal([batch_size, self.latent_dim])

      with tf.GradientTape(persistent=True) as tape:
          # Generate fake images
          generated_images = self.generator(noise, training=True)

          # Discriminate image
          real_output = self.discriminator(images, training=True)
          fake_output = self.discriminator(generated_images, training=True)

          # Discriminator loss
          disc_loss = self.discriminator_loss(real_output, fake_output)
          # Generator loss
          gen_loss = self.generator_loss(fake_output)

      # Calculate gradients
      disc_grad = tape.gradient(disc_loss, self.discriminator.trainable_variables)
      gen_grad = tape.gradient(gen_loss, self.generator.trainable_variables)

      # Update discriminator weights
      self.discriminator_optimizer.apply_gradients(zip(disc_grad, self.discriminator.trainable_variables))

      # Update generator weights
      self.generator_optimizer.apply_gradients(zip(gen_grad, self.generator.trainable_variables))

      return disc_loss, gen_loss

  # @tf.function
  # def train_step(self, images):
  #     # Generate random noise
  #     batch_size = images.shape[0]
  #     noise = tf.random.normal([batch_size, self.latent_dim], dtype=dtype_global)

  #     with tf.GradientTape(persistent=False) as tape:
  #         # Generate fake images
  #         generated_images = self.generator(noise, training=False)

  #         # Discriminate image
  #         real_output = self.discriminator(images, training=True)
  #         fake_output = self.discriminator(generated_images, training=True)

  #         # Discriminator loss
  #         disc_loss = self.discriminator_loss(real_output, fake_output)

  #     if disc_loss < 1.e-10:
  #       # Calculate disc gradients
  #       disc_grad = tape.gradient(disc_loss, self.discriminator.trainable_variables)
  #       # Update discriminator weights
  #       self.discriminator_optimizer.apply_gradients(zip(disc_grad, self.discriminator.trainable_variables))

  #     with tf.GradientTape(persistent=False) as tape:
  #         # Generate fake images
  #         generated_images = self.generator(noise, training=True)

  #         # Discriminate image
  #         fake_output = self.discriminator(generated_images, training=False)

  #         # Generator loss
  #         gen_loss = self.generator_loss(fake_output)

  #     if gen_loss < 1.e-10:
  #       # Calculate gen gradients
  #       gen_grad = tape.gradient(gen_loss, self.generator.trainable_variables)
  #       # Update generator weights
  #       self.generator_optimizer.apply_gradients(zip(gen_grad, self.generator.trainable_variables))

  #     # printGUPMemoryUsage()

  #     return disc_loss, gen_loss

  # # # Define the training step
  # @tf.function
  # def train_step(self, images):
  #     # Generate random noise
  #     batch_size = images.shape[0]
  #     noise = tf.random.normal([batch_size, self.latent_dim], dtype=dtype_global)

  #     with tf.GradientTape(persistent=True) as tape:
  #         # Generate fake images
  #         generated_images = self.generator(noise, training=True)

  #         # Discriminate image
  #         real_output = self.discriminator(images, training=True)
  #         fake_output = self.discriminator(generated_images, training=True)

  #         # Discriminator loss
  #         disc_loss = self.discriminator_loss(real_output, fake_output)
  #         # Generator loss
  #         gen_loss = self.generator_loss(fake_output)

  #     # Calculate disc gradients
  #     disc_grad = tape.gradient(disc_loss, self.discriminator.trainable_variables)
  #     # Update discriminator weights
  #     self.discriminator_optimizer.apply_gradients(zip(disc_grad, self.discriminator.trainable_variables))

  #     # Calculate gen gradients
  #     gen_grad = tape.gradient(gen_loss, self.generator.trainable_variables)
  #     # Update generator weights
  #     self.generator_optimizer.apply_gradients(zip(gen_grad, self.generator.trainable_variables))

  #     return disc_loss, gen_loss

  def train(self, dataset, epochs):
    gen_loss_list = []
    disc_loss_list = []
    epoch_list = []
    for epoch in range(epochs):
      start = time.time()

      for i, image_batch in enumerate(dataset):
        disc_loss, gen_loss = self.train_step(image_batch)
        print('{}th epoch {}th batch >>> Disc loss: {} , Gen loss: {}'.format(epoch+1, i, disc_loss, gen_loss))
        display.clear_output(wait=True)

      # Save the model every 10 epochs
      if (epoch + 1) % 10 == 0:
        print('Model saved:', self.save())

      print ('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))
      print('Disc loss: {} , Gen loss: {}'.format(disc_loss, gen_loss))

      epoch_list.append(epoch+1)
      gen_loss_list.append(gen_loss)
      disc_loss_list.append(disc_loss)

    return np.array([epoch_list, gen_loss_list, disc_loss_list])



class SelfAttention_layer(layers.Layer):
  '''Self Attention.
    Args
    ----------
      dim : Int. The input and out dimension of per token features.
      num_heads: : Int. Number of attention heads.
      qkv_bias: Boolean, whether the dense layers use bias vectors/matrices in MultiHeadAttention.
      attn_p : Float. Dropout probability applied to the query, key and value tensors.
      proj_p : Float. Dropout probability applied to the output tensor.
  '''

  def __init__(self, dim:int, num_heads:int=4, qkv_bias:bool=True, attn_p:float=0., proj_p:float=0.):
    super().__init__()
    self.num_heads:int = num_heads
    self.dim:int = dim
    self.head_dim:int = dim // num_heads
    self.scale:float = self.head_dim ** -0.5

    self.qkv_layer = tf.keras.layers.Dense(dim*3, use_bias=qkv_bias, input_shape=(None, dim), dtype=dtype_global)

    if attn_p > 0.0:
      self.attn_drop_layer = tf.keras.layers.Dropout(attn_p, dtype=dtype_global)
    else:
      self.attn_drop_layer = tf.keras.layers.Identity(dtype=dtype_global)

    self.proj_layer = tf.keras.layers.Dense(dim, input_shape=(None, dim), dtype=dtype_global)

    if proj_p > 0.0:
      self.proj_drop_layer = tf.keras.layers.Dropout(proj_p, dtype=dtype_global)
    else:
      self.proj_drop_layer = tf.keras.layers.Identity(dtype=dtype_global)

    # print('---'*10)
    # print('dtype discription: SelfAttention init')
    # printGUPMemoryUsage()
    # print('layer    : dtpe >>> policy >>> compute')
    # print('qkv_layer:', self.qkv_layer.dtype, self.qkv_layer.dtype_policy, self.qkv_layer.compute_dtype)
    # print('attn_drop:', self.attn_drop_layer.dtype, self.attn_drop_layer.dtype_policy, self.attn_drop_layer.compute_dtype)
    # print('proj_layr:', self.proj_layer.dtype, self.proj_layer.dtype_policy, self.proj_layer.compute_dtype)
    # print('---'*10)


  def call(self, input):
    '''
      Args:
          input: Tensor whith the shape `(batch_size, num_patches, dim)`.

      Returns
          output: Tensor with the shape `(batch_size, num_patches, dim)`.
    '''
    input = tf.cast(input, dtype=dtype_global)

    b, n, c = input.shape
    qkv = self.qkv_layer(input)  # (batch_size, num_patches, dim*3)
    qkv = tf.reshape(qkv, (b, n, 3, self.num_heads, self.head_dim)) # (batch_size, num_patches, 3, num_heads, head_dim)
    qkv = tf.transpose(qkv, (2, 0, 3, 1, 4)) # (3, batch_size, num_heads, num_patches, head_dim)
    q, k, v = qkv[0], qkv[1], qkv[2]

    # Scaled-dot product
    x = tf.matmul(q, k, transpose_b=True) * self.scale # (batch_size, num_heads, num_patches, num_patches)

    # Calc attention weight
    x = tf.nn.softmax(x, axis=-1)

    # Drop out applied to attention weight
    x = self.attn_drop_layer(x)

    # Attention pooling
    x = tf.matmul(x, v) # (batch_size, num_heads, num_patches, head_dim)
    x = tf.transpose(x, (0, 2, 1, 3))
    x = tf.reshape(x, (b,n,c))  # (batch_size, num_patches, dim)

    x = self.proj_layer(x)  # (batch_size, num_patches, dim)
    x = self.proj_drop_layer(x)

    # print('==='*10)
    # print('dtype discription: SelfAttention call')
    # printGUPMemoryUsage()
    # print('input :', input.dtype)
    # print('output:', x.dtype)

    return x

class CrossAttention_layer(layers.Layer):
  '''Cross Attention.
    Args
    ----------
      que_dim : Int. The input and out dimension of query.
      key_dim : Int. The input dimension of key.
      num_heads: : Int. Number of attention heads.
      qkv_bias: Boolean, whether the dense layers use bias vectors/matrices in MultiHeadAttention.
      attn_p : Float. Dropout probability applied to the query, key and value tensors.
      proj_p : Float. Dropout probability applied to the output tensor.
  '''

  def __init__(self, que_dim:int, key_dim:int, num_heads:int=4, qkv_bias:bool=True, attn_p:float=0., proj_p:float=0.):
    super().__init__()
    self.num_heads:int = num_heads
    self.que_dim:int = que_dim
    self.key_dim:int = key_dim
    self.head_dim:int = que_dim // num_heads
    self.scale:float = self.head_dim ** -0.5

    self.q_layer = tf.keras.layers.Dense(que_dim, use_bias=qkv_bias, input_shape=(None, que_dim), dtype=dtype_global)
    self.k_layer = tf.keras.layers.Dense(que_dim, use_bias=qkv_bias, input_shape=(None, key_dim), dtype=dtype_global)
    self.v_layer = tf.keras.layers.Dense(que_dim, use_bias=qkv_bias, input_shape=(None, key_dim), dtype=dtype_global)

    if attn_p > 0.0:
      self.attn_drop_layer = tf.keras.layers.Dropout(attn_p, dtype=dtype_global)
    else:
      self.attn_drop_layer = tf.keras.layers.Identity(dtype=dtype_global)

    self.proj_layer = tf.keras.layers.Dense(que_dim, input_shape=(None, que_dim), dtype=dtype_global)

    if proj_p > 0.0:
      self.proj_drop_layer = tf.keras.layers.Dropout(proj_p, dtype=dtype_global)
    else:
      self.proj_drop_layer = tf.keras.layers.Identity(dtype=dtype_global)

  def call(self, input, embedding):
    '''
      Args:
          input: query. Tensor whith the shape `(batch_size, num_patches, que_dim)`.
          embedding: key. Tensor whith the shape `(batch_size, num_patches, key_dim)`.

      Returns
          output: Tensor with the shape `(batch_size, num_patches, dim)`.
    '''
    input = tf.cast(input, dtype=dtype_global)
    embedding = tf.cast(embedding, dtype=dtype_global)

    b, n, c = input.shape
    b, e_n, e_c = embedding.shape


    q = self.q_layer(input)  # (batch_size, num_patches, que_dim)
    q = tf.reshape(q,(b, n, self.num_heads, self.head_dim))
    q = tf.transpose(q, (0, 2, 1, 3)) # (batch_size, num_heads, num_patches, head_dim)

    k = self.k_layer(embedding)  # (batch_size, num_patches, key_dim)
    k = tf.reshape(k,(b, e_n, self.num_heads, self.head_dim))
    k = tf.transpose(k, (0, 2, 1, 3)) # (batch_size, num_heads, num_embed, head_dim)

    v = self.k_layer(embedding)  # (batch_size, num_patches, key_dim)
    v = tf.reshape(v,(b, e_n, self.num_heads, self.head_dim))
    v = tf.transpose(v, (0, 2, 1, 3)) # (batch_size, num_heads, num_embed, head_dim)

    # Scaled-dot product
    x = tf.matmul(q, k, transpose_b=True) * self.scale # (batch_size, num_heads, num_patches, num_embed)

    # Calc attention weight
    x = tf.nn.softmax(x, axis=-1)

    # Drop out applied to attention weight
    x = self.attn_drop_layer(x)

    # Attention pooling
    x = tf.matmul(x, v) # (batch_size, num_heads, num_patches, head_dim)
    x = tf.transpose(x, (0, 2, 1, 3))
    x = tf.reshape(x, (b, n, self.que_dim))  # (batch_size, num_patches, que_dim)

    x = self.proj_layer(x)  # (batch_size, num_patches, que_dim)
    x = self.proj_drop_layer(x)

    return input + x


class MLP_layer(layers.Layer):
  r"""Implement multilayer perceptron (MLP)

    Args:
      hiddden_units: List of output dimension for each dense layer.
      activation: String. Activation function to use.
      dropout_rate: Float. Dropout rate.

  """
  def __init__(self, hidden_units:list, activation:str='gelu', dropout_rate:float=0.):
    super().__init__()
    self.dropout_rate:float = dropout_rate
    self.dense_layers:list = []

    for units in hidden_units:
      self.dense_layers.append(layers.Dense(units, activation=activation, dtype=dtype_global))
      if dropout_rate > 0.0:
        self.dense_layers.append(layers.Dropout(dropout_rate, dtype=dtype_global))

  def call(self, input):
    x = tf.cast(input, dtype=dtype_global)

    for layer in self.dense_layers:
        x = layer(x)
    return x

class MLP_layer_2x(layers.Layer):
  r"""Implement multilayer perceptron (MLP)

    Args:
      hiddden_units: List of output dimension for each dense layer.
      activation: String. Activation function to use.
      dropout_rate: Float. Dropout rate.

  """
  def __init__(self, hidden_units:list, activation:str='gelu', dropout_rate:float=0., in_dim:int=None, ):
    super().__init__()
    in_dim = in_dim or hidden_units[0]
    self.dropout_rate:float = dropout_rate
    self.dense_1 = layers.Dense(hidden_units[0], activation=activation, input_shape=(None, in_dim), dtype=dtype_global)
    if dropout_rate > 0.0:
      self.drop_1 = layers.Dropout(dropout_rate, dtype=dtype_global)
      self.drop_2 = layers.Dropout(dropout_rate, dtype=dtype_global)
    else:
      self.drop_1 = layers.Identity(dtype=dtype_global)
      self.drop_2 = layers.Identity(dtype=dtype_global)
    self.dense_2 = layers.Dense(hidden_units[1], activation=None, input_shape=(None, hidden_units[0]), dtype=dtype_global)

    # print('---'*10)
    # print('dtype discription: class MLP_layer_2x init')
    # printGUPMemoryUsage()
    # print('layer    : dtpe >>> policy >>> compute')
    # print('dense_1  :', self.dense_1.dtype, self.dense_1.dtype_policy, self.dense_1.compute_dtype)
    # print('drop_2   :', self.drop_2.dtype, self.drop_2.dtype_policy, self.drop_2.compute_dtype)
    # print('dense_2  :', self.dense_2.dtype, self.dense_2.dtype_policy, self.dense_2.compute_dtype)
    # print('---'*10)


  def call(self, input):
    x = tf.cast(input, dtype=dtype_global)

    # print('==='*10)
    # print('dtype discription: class MLP_layer_2x call')
    # printGUPMemoryUsage()
    # print('layer    : dtpe >>> policy >>> compute')
    # print('inut x   :', x.dtype)

    x = self.dense_1(x)
    x = self.drop_1(x)
    x = self.dense_2(x)
    x = self.drop_2(x)

    # print('out  x   :', x.dtype)
    return x

def drop_path(input, drop_prob:float=0., training:bool=False):
    x = tf.cast(input, dtype=dtype_global)

    if drop_prob == 0. or not training:
        return x
    keep_prob:float = 1 - drop_prob
    shape = tf.shape(x)
    random_tensor = keep_prob + tf.random.uniform(shape, dtype=x.dtype)
    random_tensor = tf.floor(random_tensor)  # binarize
    output = tf.math.divide(x, keep_prob) * random_tensor
    return output

class DropPath(tf.keras.layers.Layer):
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def call(self, input, training=False):
        x = tf.cast(input, dtype=dtype_global)
        return drop_path(x, self.drop_prob, training)

class WindowPartition_layer(layers.Layer):
  """
  Args:
      window_size (int): window size
      x: (B, H, W, C)
  Returns:
      windows: (num_windows*B, window_size*window_size, C)
  """
  def __init__(self, window_size:int):
    super().__init__()
    self.window_size:int = window_size

  def call(self,input):
    """
      Args:
        x: (B, H, W, C)
      Returns:
        windows: (num_windows*B, window_size*window_size, C)
    """
    x = tf.cast(input, dtype=dtype_global)

    B, H, W, C = x.shape
    x = tf.reshape(x, (B, H//self.window_size, self.window_size, W//self.window_size, self.window_size, C))
    x = tf.transpose(x,(0, 1, 3, 2, 4, 5))
    x = tf.reshape(x, (-1, self.window_size, self.window_size, C))
    x = tf.reshape(x, (-1, self.window_size*self.window_size, C))
    return x

class WindowReverse_layer(layers.Layer):
  def __init__(self, window_size:int, H:int, W:int):
    """
    Args:
        windows: (num_windows*B, window_size*window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image
    Returns:
        x: (B, H, W, C)
    """
    super().__init__()
    self.window_size:int = window_size
    self.H:int = H
    self.W:int = W

  def call(self, input):
    """
    Args:
      x: (num_windows*B, window_size*window_size, C)
    Returns:
      x: (B, H, W, C)
    """
    x = tf.cast(input, dtype=dtype_global)

    C = x.shape[-1]
    num_windows:int = (self.H//self.window_size)*(self.W//self.window_size)
    B:int = x.shape[0] // num_windows

    x = tf.reshape(x, (-1, self.window_size, self.window_size, C))
    x = tf.reshape(x, (B, self.H//self.window_size, self.W//self.window_size, self.window_size, self.window_size, -1))
    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))
    x = tf.reshape(x, (B, self.H, self.W, -1))
    return x

class BlockGridTrans(layers.Layer):
  '''Transformer block for generator.

    Args:
        que_dim : Int. The input and out dimension of query.
        key_dim : Int. The input dimension of key.
        num_heads: Int. Number of attention heads.
        mlp_ratio: Int, the factor for determination of the hidden dimension size of the MLP module with respect
        to que_dim.
        mlp_p: Float. Dropout probability applied to the MLP.
        qkv_bias: Boolean, whether the dense layers use bias vectors/matrices in Attention.
        attn_p : Float. Dropout probability applied to Attention.
        activation: String. Activation function to use in MLP.
        window_size: Int, grid size for grid transformer block.

  '''
  def __init__(self, que_dim:int, key_dim:int, num_heads:int=4, mlp_ratio:int=4, mlp_p:float=0., qkv_bias:bool=False, attn_p:float=0., proj_p:float=0., activation:str='gelu', window_size:float=16):
    super().__init__()

    self.window_size = window_size
    self.norm1 = layers.LayerNormalization(epsilon=1e-6, dtype=dtype_global)
    self.norm2 = layers.LayerNormalization(epsilon=1e-6, dtype=dtype_global)
    hidden_units = [que_dim * mlp_ratio, que_dim]
    self.mlp = MLP_layer(hidden_units=hidden_units, dropout_rate=mlp_p, activation=activation)

    # self.cross_attention = CrossAttention_layer(que_dim=que_dim, key_dim=key_dim, num_heads=num_heads, qkv_bias=qkv_bias)
    self.attention = SelfAttention_layer(dim=que_dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_p=attn_p, proj_p=proj_p)

  def call(self, input):
    x, embedding = input

    x = self.cross_attention(x, embedding)

    B, N, C = x.shape
    H = W = int(np.sqrt(N))

    x = layers.Reshape((H, W, C), dtype=dtype_global)(x)
    x = WindowPartition_layer(self.window_size)(x)

    x = x + self.attention(self.norm1(x))

    x = WindowReverse_layer(self.window_size, H, W)(x)
    x = layers.Reshape((N, C), dtype=dtype_global)(x)

    x = x + self.mlp(self.norm2(x))

    return [x, embedding]

class Block(layers.Layer):
  '''Transformer block for generator.

    Args:
        que_dim: Int. Embeddinig dimension.
        num_heads: Int. Number of attention heads.
        mlp_ratio: Int, the factor for determination of the hidden dimension size of the MLP module with respect
        to embed_dim.
        mlp_p: Float. Dropout probability applied to the MLP.
        qkv_bias: Boolean, whether the dense layers use bias vectors/matrices in Attention.
        attn_p : Float. Dropout probability applied to Attention.
        proj_p : Float. Dropout probability applied to the output tensor.
        activation: String. Activation function to use in MLP.

  '''
  def __init__(self, que_dim:int, num_heads:int=4, mlp_ratio:int=4, mlp_p:float=0., qkv_bias:bool=False, attn_p:float=0., proj_p:float=0., activation:str='gelu'):
    super().__init__()
    self.norm1 = layers.LayerNormalization(epsilon=1e-6, dtype=dtype_global)
    self.norm2 = layers.LayerNormalization(epsilon=1e-6, dtype=dtype_global)
    self.attention = SelfAttention_layer(dim=que_dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_p=attn_p, proj_p=proj_p)
    hidden_units = [que_dim * mlp_ratio, que_dim]
    # self.mlp = MLP_layer(hidden_units=hidden_units, dropout_rate=mlp_p, activation=activation)
    self.mlp = MLP_layer_2x(hidden_units=hidden_units, dropout_rate=mlp_p, activation=activation, in_dim=que_dim)

    # print('---'*10)
    # print('dtype discription: class Block init')
    # printGUPMemoryUsage()
    # print('norm dtype:',self.norm1.dtype,' >>>',self.norm1.dtype_policy,' >>> compute:',self.norm1.compute_dtype)
    # print('attention :      >>>',self.attention.dtype_policy,' >>> compute:',self.attention.compute_dtype)
    # print('mlp dtype :      >>>',self.mlp.dtype_policy,' >>> compute:',self.mlp.compute_dtype)
    # print('---'*10)

  def call(self, input):
    x = tf.cast(input,dtype=dtype_global)
    x = x + self.attention(self.norm1(x))
    x = x + self.mlp(self.norm2(x))

    # print('==='*10)
    # print('dtype discription: class Block call')
    # printGUPMemoryUsage()
    # print('input :', input.dtype)
    # print('output:', x.dtype)
    return x

class StageBlock(layers.Layer):
  '''Transformer block Stage for generator.

    Args:
        depth: Int. The number of transformer blocks in the stage.
        que_dim: Int. Embeddinig dimension.
        num_heads: Int. Number of attention heads.
        mlp_ratio: Int, the factor for determination of the hidden dimension size of the MLP module with respect
        to embed_dim.
        mlp_p: Float. Dropout probability applied to the MLP.
        qkv_bias: Boolean, whether the dense layers use bias vectors/matrices in Attention.
        attn_p : Float. Dropout probability applied to Attention.
        activation: String. Activation function to use in MLP.

  '''

  def __init__(self, depth:int, que_dim:int, num_heads:int=4, mlp_ratio:int=4, mlp_p:float=0., qkv_bias:bool=False, attn_p:float=0., proj_p:float=0., activation:str='gelu'):
    super().__init__()


    self.blocks = [
        Block(
            que_dim=que_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]

  def call(self, input):

    x = tf.cast(input, dtype=dtype_global)
    # print('dtype mid   :', x.dtype)
    for block in self.blocks:
      x = block(x)

    # print('==='*10)
    # print('dtype discription: StageBlock call')
    # printGUPMemoryUsage()
    # print('dtype input :', input.dtype)
    # print('dtype output:', x.dtype)
    return x


class PixelShuffle_layer(layers.Layer):
  r"""Implementation of the PixelShuffle layer

  PixelShuffle is a technique that can be used for upsampling in deep learning,
  particularly in computer vision applications.
  It was introduced in a paper titled
  "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network"
  by Shi et al. in 2016.

  Arguments:
    scale : specifies the upsampling factor.
  """

  def __init__(self, scale:int, kernel_size = 3):
    super().__init__()
    self.scale = scale
    self.kernel_size = kernel_size

  def call(self, input):
    batch_size, height, width, in_channels = input.shape
    out_channels = in_channels // (self.scale**2)

    x = tf.cast(input, dtype=dtype_global)

    # Reshape to (batch_size, height, width, scale, scale, out_channels)
    x = tf.reshape(x, [batch_size, height, width, self.scale, self.scale, out_channels])

    # Transpose to (batch_size, height*scale, width*scale, out_channels)
    x = tf.transpose(x, [0, 1, 3, 2, 4, 5])
    x = tf.reshape(x, [batch_size, height * self.scale, width * self.scale, out_channels])

    # print('==='*10)
    # print('dtype discription: PixelShufll call')
    # printGUPMemoryUsage()
    # print('input :', input.dtype)
    # print('output:', x.dtype)

    return x

class Generator(tf.keras.Model):

  r'''Generator with transformer blocks.

    Args:
        depth: Int. The number of transformer blocks in the stage.
        que_dim: Int. Embeddinig dimension.
        num_heads: Int. Number of attention heads.
        mlp_ratio: Int, the factor for determination of the hidden dimension size of the MLP module with respect
        to embed_dim.
        mlp_p: Float. Dropout probability applied to the MLP.
        qkv_bias: Boolean, whether the dense layers use bias vectors/matrices in Attention.
        attn_p : Float. Dropout probability applied to Attention.
        activation: String. Activation function to use in MLP.

  '''
  def __init__(self, latent_dim:int=100, depth:list=[5,4,2], que_dim:int=1024, num_heads:int=4, mlp_ratio:float=4, mlp_p:float=0., qkv_bias:bool=False, attn_p:float=0., proj_p:float=0., activation:str='gelu', bottom_width:int=8):
    super().__init__()

    self.latent_dim = latent_dim
    self.bottom_width = bottom_width
    self.embed_dim = embed_dim = que_dim
    self.dense_1 = layers.Dense((self.bottom_width ** 2) * self.embed_dim, input_shape=(None,latent_dim), dtype=dtype_global)


    self.pos_embed_1 = tf.Variable(tf.zeros((1, bottom_width**2, embed_dim), dtype=dtype_global))
    self.pos_embed_2 = tf.Variable(tf.zeros((1, (bottom_width*2)**2, embed_dim//4), dtype=dtype_global))
    self.pos_embed_3 = tf.Variable(tf.zeros((1, (bottom_width*4)**2, embed_dim//16), dtype=dtype_global))
    self.pos_embed = [
        self.pos_embed_1,
        self.pos_embed_2,
        self.pos_embed_3
    ]

    self.blocks = StageBlock(
        depth=depth[0],
        que_dim = que_dim,
        num_heads = num_heads,
        mlp_ratio = mlp_ratio,
        mlp_p = mlp_p,
        qkv_bias = qkv_bias,
        attn_p = attn_p,
        activation = activation
    )
    # self.blocks = layers.Identity()

    self.upsample_blocks = [
        StageBlock(
            depth=depth[1],
            que_dim = que_dim//4,
            num_heads = num_heads,
            mlp_ratio = mlp_ratio,
            mlp_p = mlp_p,
            qkv_bias = qkv_bias,
            attn_p = attn_p,
            activation = activation
        ),
        StageBlock(
            depth=depth[2],
            que_dim = que_dim//16,
            num_heads = num_heads,
            mlp_ratio = mlp_ratio,
            mlp_p = mlp_p,
            qkv_bias = qkv_bias,
            attn_p = attn_p,
            activation = activation
        )
    ]

    self.conv_1 = layers.Conv2D(3, 1, 1, padding='valid', activation='tanh', dtype=dtype_global)

    # print('----'*10)
    # print('dtype discription: Generator __inti___')
    # printGUPMemoryUsage()
    # print('dense dtyp:',self.dense_1.dtype,'>>> policy:',self.dense_1.dtype_policy, '>>> compute:',self.dense_1.compute_dtype)
    # print('blocke dtyp:     >>> policy:',self.blocks.dtype_policy, '>>> compute:',self.blocks.compute_dtype)
    # print('conv dtyp:',self.conv_1.dtype,'>>> policy:',self.conv_1.dtype_policy, '>>> compute:',self.conv_1.compute_dtype)
    # print('----'*10)

  def call(self, latent_vector):
    latent_vector = tf.cast(latent_vector, dtype=dtype_global)

    batch_size = latent_vector.shape[0]
    H = self.bottom_width
    W = self.bottom_width
    C = self.embed_dim

    x = tf.reshape(self.dense_1(latent_vector), (-1, H*W, C))
    # x = layers.Reshape( (H*W, C), dtype=dtype_global)(self.dense_1(latent_vector))
    x = x + self.pos_embed[0]
    x = self.blocks(x)

    # print('===='*10)
    # print('CMD execution: Generator call')
    # printGUPMemoryUsage()
    # print('dtype input :', latent_vector.dtype)
    # print('dtype x     :',x.dtype)
    # print('command being executed -- ','blocks')

    for i, block in enumerate(self.upsample_blocks):
      x = tf.reshape(x, (batch_size, H, W, C))
      # x = layers.Reshape((H, W, C), dtype=dtype_global)(x)

      x = PixelShuffle_layer(2)(x)

      _, H, W, C = x.shape
      # x = tf.reshape(x, (batch_size, -1, C))
      x = layers.Reshape((-1, C), dtype=dtype_global)(x)

      x = x + self.pos_embed[i+1]
      x = block(x)

    x = tf.reshape(x, (batch_size, H, W, C))
    # x = layers.Reshape((H, W, C), dtype=dtype_global)(x)
    x = self.conv_1(x)

    # print('dtype out:',x.dtype, 'shape:', x.shape)
    # printGUPMemoryUsage()
    # print('===='*10)

    return x


class PatchEmbed_layer(layers.Layer):
  '''Divide an image into patches

    As ViT operates on sequences, you need to tokenize the images into sequences of tokens.
    Each token represents a specific region or patch of the image. You can divide the image
    into a grid of patches and flatten each patch into a single token. Additionally,
    add a special "classification" token at the beginning to indicate the task.

    The approach to divide an image into patches is by using a convolutional layer (Conv2D) as part of the tokenization process.
    This approach can be seen as a form of patch extraction using a sliding window technique.

     Sliding Window Patch Extraction: Use a Conv2D layer with a sliding window approach to extract patches from the image.
     The Conv2D layer acts as a local feature extractor, similar to how it is used in convolutional neural networks.

      > Configure the Conv2D layer with appropriate kernel size, stride, and padding.
      > The kernel size determines the patch size you want to extract.
      > The stride determines the amount of overlap between patches.
      > Padding can be used to ensure that patches are extracted from the entire image.

    Positional Encoding: As ViT doesn't encode spatial information inherently, you need to include positional encodings for each token.
    The positional encodings represent the position or order of the tokens within the sequence. You can use sine and cosine functions or
    learnable embeddings to generate these positional encodings.


  Args:
    image_size: Tuple. Size of the image wiht the shape (Height, Width).
    patch_size: Int. Size of the patch.
    num_patches: the number of patches in one image
    embed_dim: the size of a vector that each patch is projected.
    kernel_size: Int or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.

  Returns:
    A tf.tensor with the shape of (batches, num patches, embed dimension)

  '''

  def __init__(self, image_size, patch_size, embed_dim, kernel_size, padding='valid', add_pos=False):
    super().__init__()
    self.embed_dim = embed_dim
    self.num_patches = num_patches = (image_size[0]//patch_size)*(image_size[1]//patch_size)
    self.add_pos = add_pos

    # Split into patches and Project to a vector with embed dimensional.
    self.projection = tf.keras.layers.Conv2D(filters=embed_dim, kernel_size=kernel_size, strides=patch_size, padding=padding, dtype=dtype_global)

    # Position Embed
    if add_pos:
      self.pos_embed = tf.Variable(tf.zeros((1, num_patches, embed_dim), dtype=dtype_global))

  def call(self, images):
    # batch_size = tf.shape(images)[0]
    batch_size = images.shape[0]
    images = tf.cast(images, dtype=dtype_global)

    x = self.projection(images)
    x = tf.reshape(x, (batch_size, -1, self.embed_dim))
    if self.add_pos:
      x = x + self.pos_embed
    return x

class AddPositionEmbed_layer(layers.Layer):
  def __init__(self, num_patches, embed_dim):
    super().__init__()
    self.pos_embed = tf.Variable(tf.zeros((1, num_patches, embed_dim), dtype=dtype_global))

  def call(self, input):
    x = tf.cast(input, dtype=dtype_global)
    return x + self.pos_embed

class AddCLSToken_layer(layers.Layer):
  def __init__(self, embed_dim):
    super().__init__()
    self.cls_token = tf.Variable(tf.zeros((1, 1, embed_dim), dtype=dtype_global))

  def call(self, input):
    x = tf.cast(input, dtype=dtype_global)
    batch_size = x.shape[0]
    return tf.concat([tf.tile(self.cls_token, [batch_size, 1, 1]), x], axis=1)


class discriminator(tf.keras.Model):
  '''Discriminator with a Vision Transformer (ViT).
  Building a discriminator with a Vision Transformer (ViT) involves adapting
  the transformer architecture to perform binary classification tasks on image data.

  In a Vision Transformer (ViT), the primary components include self-attention mechanisms,
  transformer encoder layers, position embeddings, and classification heads. Each component:

  Self-Attention Mechanisms: Self-attention is a key component in transformers, including ViTs.
    Self-attention allows the model to capture relationships between different tokens within
    a sequence. It calculates attention weights for each token based on its relation to other tokens,
    enabling the model to focus on relevant information during processing.

  Transformer Encoder Layers: ViTs consist of multiple transformer encoder layers stacked on
    top of each other. Each transformer encoder layer consists of two sub-layers:
    a multi-head self-attention mechanism and a feed-forward neural network.
    The self-attention mechanism captures the dependencies between tokens,
    while the feed-forward network applies non-linear transformations to each token independently.

  Position Embeddings: Position embeddings encode the spatial information of each token within
    the image sequence. Since ViTs do not have built-in positional information like CNNs, position
    embeddings are added to the input tokens to indicate their relative positions. Commonly,
    sine and cosine functions or learnable embeddings are used to generate position embeddings.

  Classification Head: A classification head is added to the output of the ViT model to perform
    the final task-specific prediction. For binary classification tasks like discrimination,
    the classification head typically consists of a fully connected layer followed by
    a sigmoid activation function to produce the binary classification output.

    Args:
      image_size: Turple.
      depth: Int. The number of transformer blocks.
      num_classes: Int. The number of classes.
      embed_dim: Int. Embeddinig dimension.
      patch_siz: Int.
      mlp_p: Float. Dropout probability applied to the MLP.
      num_heads: Int. Number of attention heads.
      mlp_ratio: Int, the factor for determination of the hidden dimension size of the MLP module with respect to embed_dim.
      attn_p : Float. Dropout probability applied to MultiHeadAttention.
      window_size: Int, grid size for grid transformer block.


  '''

  def __init__(
      self,
      image_size = (64, 64),
      depth = 3,
      num_classes = 1,
      embed_dim = 384,
      patch_size = 2,
      mlp_p = 0.,
      num_heads = 4,
      mlp_ratio = 4,
      attn_p = 0.,
      window_size = 4,
      ):

    super().__init__()

    self.patch_size = patch_size

    self.embed_dim = embed_dim
    self.embed_dim_1 = embed_dim_1 = embed_dim//4
    self.embed_dim_2 = embed_dim_2 = embed_dim//4
    self.embed_dim_3 = embed_dim_3 = embed_dim//2

    self.patch_size_1 = patch_size_1 = patch_size
    self.patch_size_2 = patch_size_2 = patch_size*2
    self.patch_size_3 = patch_size_3 = patch_size*4

    self.patches_1 = PatchEmbed_layer(image_size, patch_size = patch_size_1, embed_dim = embed_dim_1, kernel_size = patch_size_1*2, padding='same')
    self.patches_2 = PatchEmbed_layer(image_size, patch_size = patch_size_2, embed_dim = embed_dim_2, kernel_size = patch_size_2, padding='valid')
    self.patches_3 = PatchEmbed_layer(image_size, patch_size = patch_size_3, embed_dim = embed_dim_3, kernel_size = patch_size_3, padding='valid')

    num_patches_1 = self.patches_1.num_patches
    # num_patches_2 = self.patches_2.num_patches
    # num_patches_3 = self.patches_3.num_patches

    self.add_pos_embed_1 = AddPositionEmbed_layer(num_patches_1, embed_dim_1)
    # self.add_pos_embed_2 = AddPositionEmbed_layer(num_patches_2, embed_dim_2)
    # self.add_pos_embed_3 = AddPositionEmbed_layer(num_patches_3, embed_dim_3)

    self.window_size = window_size

    self.blocks_1 = [
        Block(
            que_dim=embed_dim_1, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]

    self.blocks_2 = [
        Block(
            que_dim=embed_dim_1+embed_dim_2, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth-1)
    ]

    self.blocks_21 = [
        Block(
            que_dim=embed_dim_1+embed_dim_2, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(1)
    ]

    self.blocks_3 = [
        Block(
            que_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]


    self.add_cls_token = AddCLSToken_layer(embed_dim=embed_dim)

    self.blocks_last = [
        Block(
            que_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        )
    ]

    self.layer_norm = layers.LayerNormalization(epsilon=1e-6)

    self.head = layers.Dense(num_classes)

  def call(self, input):
    batch_size, height, width, channels = input.shape
    h = height//self.patch_size_1
    w = width//self.patch_size_1

    x_1 = self.patches_1(tf.cast(input, dtype=dtype_global))
    x_2 = self.patches_2(tf.cast(input, dtype=dtype_global))
    x_3 = self.patches_3(tf.cast(input, dtype=dtype_global))

    # B, H//2*W//2, embed_dim//4 if patch_size = 2
    x = self.add_pos_embed_1(x_1)
    c = x.shape[-1]

    # -- Grid Transformer Block --
    # B, H//2, W//2, embed_dim//4 if patch_size = 2
    x = layers.Reshape((h, w, c), dtype=dtype_global)(x)
    x = WindowPartition_layer(self.window_size)(x)
    for block in self.blocks_1:
      x = block(x)
    x = WindowReverse_layer(self.window_size, h, w)(x)

    # -- 2x AvePool --
    # B, H//4, W//4, embed_dim//4
    x = layers.AveragePooling2D(2, dtype=dtype_global)(x)
    _, h, w, c = x.shape

    #  -- Concatnate --
    # B, H//4*W//4, embed_dim//4
    x = layers.Reshape((-1, c), dtype=dtype_global)(x)
    # B, H//4*W//4, embed_dim//2
    x = layers.Concatenate(axis=-1, dtype=dtype_global)([x, x_2])
    c = x.shape[-1]

    # -- Grid Transformer Block --
    x = layers.Reshape((h, w, c), dtype=dtype_global)(x)
    x = WindowPartition_layer(self.window_size)(x)
    for block in self.blocks_2:
      x = block(x)
    x = WindowReverse_layer(self.window_size, h, w)(x)
    # -- Transformer Block --
    x = layers.Reshape((h*w, c), dtype=dtype_global)(x)
    for block in self.blocks_21:
      x = block(x)

    # -- 2x AvePool --
    x = layers.Reshape((h, w, c), dtype=dtype_global)(x)
    # B, H//8, W//8, embed_dim//2
    x = layers.AveragePooling2D(2, dtype=dtype_global)(x)
    _, h, w, c = x.shape

    #  -- Concatnate --
    x = layers.Reshape((-1, c), dtype=dtype_global)(x)
    # B, H//8*W//8, embed_dim
    x = layers.Concatenate(axis=-1, dtype=dtype_global)([x, x_3])
    c = x.shape[-1]

    # -- Transformer Block --
    for block in self.blocks_3:
      x = block(x)

    # -- Add CLS token --
    # B, H//8*W//8+1, embed_dim
    x = self.add_cls_token(x)

    # -- Transformer Block --
    for block in self.blocks_last:
      x = block(x)

    x = self.layer_norm(x)
    x = self.head(x[:,0])

    return x

class discriminator_s(tf.keras.Model):
  '''Discriminator with a Vision Transformer (ViT).
  Building a discriminator with a Vision Transformer (ViT) involves adapting
  the transformer architecture to perform binary classification tasks on image data.

  In a Vision Transformer (ViT), the primary components include self-attention mechanisms,
  transformer encoder layers, position embeddings, and classification heads. Each component:

  Self-Attention Mechanisms: Self-attention is a key component in transformers, including ViTs.
    Self-attention allows the model to capture relationships between different tokens within
    a sequence. It calculates attention weights for each token based on its relation to other tokens,
    enabling the model to focus on relevant information during processing.

  Transformer Encoder Layers: ViTs consist of multiple transformer encoder layers stacked on
    top of each other. Each transformer encoder layer consists of two sub-layers:
    a multi-head self-attention mechanism and a feed-forward neural network.
    The self-attention mechanism captures the dependencies between tokens,
    while the feed-forward network applies non-linear transformations to each token independently.

  Position Embeddings: Position embeddings encode the spatial information of each token within
    the image sequence. Since ViTs do not have built-in positional information like CNNs, position
    embeddings are added to the input tokens to indicate their relative positions. Commonly,
    sine and cosine functions or learnable embeddings are used to generate position embeddings.

  Classification Head: A classification head is added to the output of the ViT model to perform
    the final task-specific prediction. For binary classification tasks like discrimination,
    the classification head typically consists of a fully connected layer followed by
    a sigmoid activation function to produce the binary classification output.

    Args:
      image_size: Turple.
      depth: Int. The number of transformer blocks.
      num_classes: Int. The number of classes.
      embed_dim: Int. Embeddinig dimension.
      patch_siz: Int.
      mlp_p: Float. Dropout probability applied to the MLP.
      num_heads: Int. Number of attention heads.
      mlp_ratio: Int, the factor for determination of the hidden dimension size of the MLP module with respect to embed_dim.
      attn_p : Float. Dropout probability applied to MultiHeadAttention.
      window_size: Int, grid size for grid transformer block.


  '''

  def __init__(
      self,
      image_size = (64, 64),
      depth = 3,
      num_classes = 1,
      embed_dim = 384,
      patch_size = 2,
      mlp_p = 0.,
      num_heads = 4,
      mlp_ratio = 4,
      attn_p = 0.,
      window_size = 4,
      ):

    super().__init__()

    self.patch_size = patch_size

    self.embed_dim = embed_dim
    self.embed_dim_1 = embed_dim_1 = embed_dim//4
    self.embed_dim_2 = embed_dim_2 = embed_dim//4
    self.embed_dim_3 = embed_dim_3 = embed_dim//2

    self.patch_size_1 = patch_size_1 = patch_size
    self.patch_size_2 = patch_size_2 = patch_size*2
    self.patch_size_3 = patch_size_3 = patch_size*4

    self.patches_1 = PatchEmbed_layer(image_size, patch_size = patch_size_1, embed_dim = embed_dim_1, kernel_size = patch_size_1*2, padding='same')
    self.patches_2 = PatchEmbed_layer(image_size, patch_size = patch_size_2, embed_dim = embed_dim_2, kernel_size = patch_size_2, padding='valid')
    self.patches_3 = PatchEmbed_layer(image_size, patch_size = patch_size_3, embed_dim = embed_dim_3, kernel_size = patch_size_3, padding='valid')

    num_patches_1 = self.patches_1.num_patches
    # num_patches_2 = self.patches_2.num_patches
    # num_patches_3 = self.patches_3.num_patches

    self.add_pos_embed_1 = AddPositionEmbed_layer(num_patches_1, embed_dim_1)
    # self.add_pos_embed_2 = AddPositionEmbed_layer(num_patches_2, embed_dim_2)
    # self.add_pos_embed_3 = AddPositionEmbed_layer(num_patches_3, embed_dim_3)

    self.window_size = window_size

    self.blocks_1 = [
        Block(
            que_dim=embed_dim_1, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]

    self.blocks_2 = [
        Block(
            que_dim=embed_dim_1+embed_dim_2, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth-1)
    ]

    self.blocks_21 = [
        Block(
            que_dim=embed_dim_1+embed_dim_2, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(1)
    ]

    self.blocks_3 = [
        Block(
            que_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]


    self.add_cls_token = AddCLSToken_layer(embed_dim=embed_dim)

    self.blocks_last = [
        Block(
            que_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        )
    ]

    self.layer_norm = layers.LayerNormalization(epsilon=1e-6)

    self.head = layers.Dense(num_classes)

  def call(self, input):
    batch_size, height, width, channels = input.shape
    h = height//self.patch_size_1
    w = width//self.patch_size_1

    x_1 = self.patches_1(tf.cast(input, dtype=dtype_global))
    x_2 = self.patches_2(tf.cast(input, dtype=dtype_global))
    x_3 = self.patches_3(tf.cast(input, dtype=dtype_global))

    # B, H//2*W//2, embed_dim//4 if patch_size = 2
    x = self.add_pos_embed_1(x_1)
    c = x.shape[-1]

    # -- Grid Transformer Block --
    # B, H//2, W//2, embed_dim//4 if patch_size = 2
    x = layers.Reshape((h, w, c), dtype=dtype_global)(x)
    x = WindowPartition_layer(self.window_size)(x)
    for block in self.blocks_1:
      x = block(x)
    x = WindowReverse_layer(self.window_size, h, w)(x)

    # -- 2x AvePool --
    # B, H//4, W//4, embed_dim//4
    x = layers.AveragePooling2D(2, dtype=dtype_global)(x)
    _, h, w, c = x.shape

    #  -- Concatnate --
    # B, H//4*W//4, embed_dim//4
    x = layers.Reshape((-1, c), dtype=dtype_global)(x)
    # B, H//4*W//4, embed_dim//2
    x = layers.Concatenate(axis=-1, dtype=dtype_global)([x, x_2])
    c = x.shape[-1]

    # -- Grid Transformer Block --
    x = layers.Reshape((h, w, c), dtype=dtype_global)(x)
    x = WindowPartition_layer(self.window_size)(x)
    for block in self.blocks_2:
      x = block(x)
    x = WindowReverse_layer(self.window_size, h, w)(x)
    # -- Transformer Block --
    x = layers.Reshape((h*w, c), dtype=dtype_global)(x)
    for block in self.blocks_21:
      x = block(x)

    # -- 2x AvePool --
    x = layers.Reshape((h, w, c), dtype=dtype_global)(x)
    # B, H//8, W//8, embed_dim//2
    x = layers.AveragePooling2D(2, dtype=dtype_global)(x)
    _, h, w, c = x.shape

    #  -- Concatnate --
    x = layers.Reshape((-1, c), dtype=dtype_global)(x)
    # B, H//8*W//8, embed_dim
    x = layers.Concatenate(axis=-1, dtype=dtype_global)([x, x_3])
    c = x.shape[-1]

    # -- Transformer Block --
    for block in self.blocks_3:
      x = block(x)

    # -- Add CLS token --
    # B, H//8*W//8+1, embed_dim
    x = self.add_cls_token(x)

    # -- Transformer Block --
    for block in self.blocks_last:
      x = block(x)

    x = self.layer_norm(x)
    x = self.head(x[:,0])

    return x

class discriminator_s(tf.keras.Model):
  '''
    Args:
      image_size: Turple.
      depth: Int. The number of transformer blocks.
      num_classes: Int. The number of classes.
      embed_dim: Int. Embeddinig dimension.
      patch_siz: Int.
      mlp_p: Float. Dropout probability applied to the MLP.
      num_heads: Int. Number of attention heads.
      mlp_ratio: Int, the factor for determination of the hidden dimension size of the MLP module with respect to embed_dim.
      attn_p : Float. Dropout probability applied to MultiHeadAttention.
      window_size: Int, grid size for grid transformer block.


  '''
  def __init__(
      self,
      image_size = (64, 64),
      depth = 3,
      num_classes = 1,
      embed_dim = 384,
      patch_size = 2,
      mlp_p = 0.,
      num_heads = 4,
      mlp_ratio = 4,
      attn_p = 0.,
      window_size = 4,
      ):

    super().__init__()

    self.patch_size = patch_size

    self.embed_dim = embed_dim
    self.embed_dim_1 = embed_dim_1 = embed_dim//4*3
    self.embed_dim_2 = embed_dim_2 = embed_dim//4

    self.patch_size_1 = patch_size_1 = patch_size
    self.patch_size_2 = patch_size_2 = patch_size*2

    self.patches_1 = PatchEmbed_layer(image_size, patch_size = patch_size_1, embed_dim = embed_dim_1, kernel_size = patch_size_1, padding='same')
    self.patches_2 = PatchEmbed_layer(image_size, patch_size = patch_size_2, embed_dim = embed_dim_2, kernel_size = patch_size_2, padding='valid')

    num_patches_1 = self.patches_1.num_patches
    # num_patches_2 = self.patches_2.num_patches
    self.add_pos_embed_1 = AddPositionEmbed_layer(num_patches_1, embed_dim_1)
    # self.add_pos_embed_2 = AddPositionEmbed_layer(num_patches_2, embed_dim_2)

    self.blocks_1 = [
        Block(
            que_dim=embed_dim_1, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]

    self.blocks_2 = [
        Block(
            que_dim=embed_dim_1+embed_dim_2, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]

    self.add_cls_token = AddCLSToken_layer(embed_dim=embed_dim)

    self.blocks_last = [
        Block(
            que_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        )
    ]

    self.layer_norm = layers.LayerNormalization(epsilon=1e-6)

    # self.head = layers.Dense(num_classes, activation='tanh')
    self.head = layers.Dense(num_classes)

  def call(self, input):
    batch_size, height, width, channels = input.shape
    h = height//self.patch_size_1
    w = width//self.patch_size_1

    x_1 = self.patches_1(tf.cast(input, dtype=dtype_global))
    x_2 = self.patches_2(tf.cast(input, dtype=dtype_global))

    # B, H//2*W//2, embed_dim//4*3 if patch_size = 2
    x = self.add_pos_embed_1(x_1)

    for block in self.blocks_1:
      x = block(x)
    c = x.shape[-1]

    # -- 2x AvePool --
    # B, H//4, W//4, embed_dim//4*3
    x = layers.Reshape((h, w, c), dtype=dtype_global)(x)
    x = layers.AveragePooling2D(2, dtype=dtype_global)(x)
    _, h, w, c = x.shape

    #  -- Concatnate --
    # B, H//4*W//4, embed_dim//4*3
    x = layers.Reshape((-1, c), dtype=dtype_global)(x)
    # B, H//4*W//4, embed_dim
    x = layers.Concatenate(axis=-1, dtype=dtype_global)([x, x_2])

    # -- Transformer Block 2--
    for block in self.blocks_2:
      x = block(x)

    # -- Add CLS token --
    # B, H//8*W//8+1, embed_dim
    x = self.add_cls_token(x)

    # -- Transformer Block --
    for block in self.blocks_last:
      x = block(x)

    x = self.layer_norm(x)
    x = self.head(x[:,0])

    return x

class discriminator_nontoken(tf.keras.Model):
  '''
    Args:
      image_size: Turple.
      depth: Int. The number of transformer blocks.
      num_classes: Int. The number of classes.
      embed_dim: Int. Embeddinig dimension.
      patch_siz: Int.
      mlp_p: Float. Dropout probability applied to the MLP.
      num_heads: Int. Number of attention heads.
      mlp_ratio: Int, the factor for determination of the hidden dimension size of the MLP module with respect to embed_dim.
      attn_p : Float. Dropout probability applied to MultiHeadAttention.
      window_size: Int, grid size for grid transformer block.


  '''
  def __init__(
      self,
      image_size = (64, 64),
      depth = 3,
      num_classes = 1,
      embed_dim = 384,
      patch_size = 2,
      mlp_p = 0.,
      num_heads = 4,
      mlp_ratio = 4,
      attn_p = 0.,
      window_size = 4,
      ):

    super().__init__()

    self.patch_size = patch_size

    self.embed_dim = embed_dim
    self.embed_dim_1 = embed_dim_1 = embed_dim//4*3
    self.embed_dim_2 = embed_dim_2 = embed_dim//4

    self.patch_size_1 = patch_size_1 = patch_size
    self.patch_size_2 = patch_size_2 = patch_size*2

    self.patches_1 = PatchEmbed_layer(image_size, patch_size = patch_size_1, embed_dim = embed_dim_1, kernel_size = patch_size_1, padding='same')
    self.patches_2 = PatchEmbed_layer(image_size, patch_size = patch_size_2, embed_dim = embed_dim_2, kernel_size = patch_size_2, padding='valid')

    num_patches_1 = self.patches_1.num_patches
    # num_patches_2 = self.patches_2.num_patches
    self.add_pos_embed_1 = AddPositionEmbed_layer(num_patches_1, embed_dim_1)
    # self.add_pos_embed_2 = AddPositionEmbed_layer(num_patches_2, embed_dim_2)

    self.blocks_1 = [
        Block(
            que_dim=embed_dim_1, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]

    self.blocks_2 = [
        Block(
            que_dim=embed_dim_1+embed_dim_2, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        ) for _ in range(depth)
    ]

    # self.add_cls_token = AddCLSToken_layer(embed_dim=embed_dim)

    self.blocks_last = [
        Block(
            que_dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, mlp_p=mlp_p, qkv_bias=False, attn_p=attn_p, activation='gelu'
        )
    ]

    self.layer_norm = layers.LayerNormalization(epsilon=1e-6)

    hidden_units = [16, 8, num_classes]
    self.head_denses = []
    for unit in hidden_units:
      self.head_denses.append(layers.Dense(unit, activation=None, dtype=dtype_global))

    # self.head_denses = [
    #     layers.Dense(16, activation=None, input_shape=(None, image_size[0]//4*image_size[1]//4*embed_dim), dtype=dtype_global),
    #     layers.Dense(8, activation=None, input_shape=(None, hidden_units[0]), dtype=dtype_global),
    #     layers.Dense(num_classes, activation=None, input_shape=(None, hidden_units[1]), dtype=dtype_global)
    #     ]

  def call(self, input):
    batch_size, height, width, channels = input.shape
    h = height//self.patch_size_1
    w = width//self.patch_size_1

    x_1 = self.patches_1(tf.cast(input, dtype=dtype_global))
    x_2 = self.patches_2(tf.cast(input, dtype=dtype_global))

    # B, H//2*W//2, embed_dim//4*3 if patch_size = 2
    x = self.add_pos_embed_1(x_1)

    for block in self.blocks_1:
      x = block(x)
    c = x.shape[-1]

    # -- 2x AvePool --
    # B, H//4, W//4, embed_dim//4*3
    x = layers.Reshape((h, w, c), dtype=dtype_global)(x)
    x = layers.AveragePooling2D(2, dtype=dtype_global)(x)
    _, h, w, c = x.shape

    #  -- Concatnate --
    # B, H//4*W//4, embed_dim//4*3
    x = layers.Reshape((-1, c), dtype=dtype_global)(x)
    # B, H//4*W//4, embed_dim
    x = layers.Concatenate(axis=-1, dtype=dtype_global)([x, x_2])

    # -- Transformer Block 2--
    for block in self.blocks_2:
      x = block(x)

    # -- Add CLS token --
    # B, H//8*W//8+1, embed_dim
    # x = self.add_cls_token(x)

    # -- Transformer Block --
    for block in self.blocks_last:
      x = block(x)

    x = self.layer_norm(x)

    x = layers.Flatten(dtype=dtype_global)(x)
    for head_dense in self.head_denses:
      x = head_dense(x)

    return x
